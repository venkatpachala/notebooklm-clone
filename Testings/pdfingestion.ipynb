{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6b97f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-6.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\venka_5gwzxwk\\appdata\\roaming\\python\\python310\\site-packages (from pypdf) (4.15.0)\n",
      "Using cached pypdf-6.7.1-py3-none-any.whl (331 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.7.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e04575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from typing import List\n",
    "\n",
    "def load_pdf(file_path: str) -> List[dict]:\n",
    "    \"\"\"Loads a pdf and ectract text page by page\n",
    "    return:\n",
    "        List of dictionaries with page content and metadata\"\"\"\n",
    "    \n",
    "    reader=PdfReader(file_path)\n",
    "    documents=[]\n",
    "\n",
    "    for page_number,page in enumerate(reader.pages):\n",
    "        text=page.extract_text()\n",
    "        if text:\n",
    "            documents.append(\n",
    "                {\n",
    "                    \"page_content\": text.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"page\": page_number +1,\n",
    "                        \"source\": file_path\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85beaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def chunk_text(\n",
    "        documents: List[Dict],\n",
    "        chunk_size: int=500,\n",
    "        overlap: int=100,\n",
    ")-> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits Documents into overlapping chunks\n",
    "    Args: \n",
    "        documents: Output from loader\n",
    "        chunk_size: Approx number of words per chunk\n",
    "        overlap: overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    chunks=[]\n",
    "    chunk_id=0\n",
    "\n",
    "    for doc in documents:\n",
    "        words=doc[\"page_content\"].split()\n",
    "\n",
    "        start=0\n",
    "        while start < len(words):\n",
    "            end=start +chunk_size\n",
    "            chunk_words = words[start:end]\n",
    "            chunk_text=\" \".join(chunk_words)\n",
    "\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": doc[\"metadata\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            chunk_id +=1\n",
    "            start +=chunk_size -overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdbf358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Pages loaded: 11\n",
      "Chunking text...\n",
      "Total chunks created: 21\n",
      "\n",
      "First chunk preview:\n",
      "GROWW PAY SERVICES PRIVATE LIMITED Vaishnavi Tech Park, South Tower, 4th Floor, Sarjapur Main Road, Bengaluru, Karnataka -560103 CIN: U67100KA2021PTC149048 email: corp.secretarial@groww.in Tel:080-69601300 Terms of Use (Last updated – May 2023) Introduction The Unified Payments Interface (“UPI”) is \n"
     ]
    }
   ],
   "source": [
    "# test_ingestion.py\n",
    "\n",
    "from ingestion.loader import load_pdf\n",
    "from ingestion.chunker import chunk_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"sample-1.pdf\" \n",
    "\n",
    "    print(\"Loading PDF...\")\n",
    "    documents = load_pdf(file_path)\n",
    "    print(f\"Pages loaded: {len(documents)}\")\n",
    "\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(documents)\n",
    "\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "    print(\"\\nFirst chunk preview:\")\n",
    "    print(chunks[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc18fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    documents: List[Dict],\n",
    "    chunk_size: int = 500,\n",
    "    overlap: int = 100,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Splits documents into overlapping chunks and assigns doc_id.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    doc_id = str(uuid.uuid4())  # unique document ID\n",
    "    chunk_id = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        words = doc[\"page_content\"].split()\n",
    "        start = 0\n",
    "\n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            chunk_words = words[start:end]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": {\n",
    "                        **doc[\"metadata\"],\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            chunk_id += 1\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c21b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ingestion"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install ingestion==0.0.24, ingestion==0.0.25, ingestion==0.0.26, ingestion==0.0.27, ingestion==0.0.28, ingestion==0.0.29, ingestion==0.0.30, ingestion==0.0.31, ingestion==0.0.32, ingestion==0.0.33, ingestion==0.0.34, ingestion==0.0.35, ingestion==0.0.36, ingestion==0.0.38, ingestion==0.0.39, ingestion==0.0.40, ingestion==0.0.41 and ingestion==0.0.42 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached ingestion-0.0.42-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting azure-keyvault-secrets==4.4.0 (from ingestion)\n",
      "  Using cached azure_keyvault_secrets-4.4.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting azure-storage-blob==12.12.0 (from ingestion)\n",
      "  Using cached azure_storage_blob-12.12.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pandas==1.4.2 (from ingestion)\n",
      "  Downloading pandas-1.4.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting openpyxl==3.0.9 (from ingestion)\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting xlrd==2.0.1 (from ingestion)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting sqlalchemy==1.4.37 (from ingestion)\n",
      "  Downloading SQLAlchemy-1.4.37-cp310-cp310-win_amd64.whl.metadata (10.0 kB)\n",
      "INFO: pip is looking at multiple versions of ingestion to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ingestion\n",
      "  Downloading ingestion-0.0.41-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.40-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.39-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.38-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.36-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.35-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.34-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is still looking at multiple versions of ingestion to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading ingestion-0.0.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.32-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.31-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.30-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.29-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading ingestion-0.0.28-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.27-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.26-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.25-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading ingestion-0.0.24-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    ingestion 0.0.42 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.41 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.40 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.39 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.38 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.36 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.35 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.34 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.33 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.32 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.31 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.30 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.29 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.28 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.27 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.26 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.25 depends on mysql-connector-python==8.0.13\n",
      "    ingestion 0.0.24 depends on mysql-connector-python==8.0.13\n",
      "\n",
      "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
      "    mysql-connector-python\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8043079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 21\n",
      "Example chunk:\n",
      "{'chunk_id': 0, 'text': 'GROWW PAY SERVICES PRIVATE LIMITED Vaishnavi Tech Park, South Tower, 4th Floor, Sarjapur Main Road, Bengaluru, Karnataka -560103 CIN: U67100KA2021PTC149048 email: corp.secretarial@groww.in Tel:080-69601300 Terms of Use (Last updated – May 2023) Introduction The Unified Payments Interface (“UPI”) is a payment platform built by National Payments Corporation of India (“NPCI”) that allows instant online payments between the bank accounts of any two parties. UPI offers an architecture and a set of standard Application Programming Interface (“API\") specifications to facilitate these online payments. It aims to simplify and provide a single interface across all NPCI systems besides creating interoperability and superior customer experience. These terms and conditions (“Terms”) regulate the payments done under UPI , a payment service platform (“Platform”) developed by NPCI, an umbrella organisation incorporated in 2008 and acting as the settlement/clearing house/regulatory agency for UPI services. We, Groww Pay Services Private Limited (“Groww”, “We”, “Us”, “Our”) are a Third party application provider (“TPAP”) authorized by NPCI to facilitate payments through “Yes Bank Limited” (“Sponsor PSP Bank”). We are a service provider, and We participate in UPI through Sponsor PSP bank. We are bound by the tripartite agreement entered with the Sponsor PSP Bank and NPCI. These Terms are an electronic record in terms of the Information Technology Act, 2000 and rules made thereunder as applicable. The Terms is (i) published in accordance with the provisions of Rule 3(1)(a) of the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021 and (ii) generated by a computer system and does not require any physical, electronic, or digital signatures by Groww. Definitions \"Applicable Law\" means all applicable laws, rules, regulations, guidelines, statutory or government notifications including Reserve Bank of India (“RBI”) regulations and various rules issued by parties involved in the payment system. “Bank Account / - Any bank account offered by any bank where (a) money can be held; (b) money can be debited from; and (c) can be credited to. “Customer’s Bank” - The bank where the End-User Customer maintains his/her account that has been linked for the purpose of debiting/crediting the payment transactions made through UPI. ‘Groww Platform” - Refers to the platform providing hosting and internet based services including website/mobile app management, API integrations, software & technological solutions, server management and other services. “Groww UPI App” - The mobile based application offered by Groww through which the UPI Services shall be provided to Users, including Merchants, hosted on Groww Platform. “Merchant” - means a person or entity with whom permitted transaction is done using Groww UPI App. “NPCI” - NPCI is a Reserve Bank of India authorized payment system operator. NPCI owns and operates the UPI payment system. \"Payment Participants\" shall mean all parties that are involved in the payment system, which inter alia, include', 'metadata': {'page': 1, 'source': '../sample-1.pdf'}}\n"
     ]
    }
   ],
   "source": [
    "from ingestion.loader import load_pdf\n",
    "from ingestion.chunker import chunk_text\n",
    "\n",
    "documents = load_pdf(\"../sample-1.pdf\")\n",
    "\n",
    "chunks = chunk_text(documents)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"Example chunk:\")\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b0ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings/embedder.py\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Converts list of texts into embedding vectors.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf84d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a39748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\venka_5gwzxwk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embedder', 'List', 'SentenceTransformer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import embeddings.embedder\n",
    "print(dir(embeddings.embedder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5343d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = r\"C:\\Users\\venka_5gwzxwk\\hf_cache\"\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c0a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: C:\\Users\\venka_5gwzxwk\\hf_cache\n",
      "HUGGINGFACE_HUB_CACHE: None\n",
      "TRANSFORMERS_CACHE: None\n",
      "HF default cache: D:\\huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE:\", os.environ.get(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "print(\"TRANSFORMERS_CACHE:\", os.environ.get(\"TRANSFORMERS_CACHE\"))\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "print(\"HF default cache:\", huggingface_hub.constants.HF_HOME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f316a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = r\"C:\\Users\\venka_5gwzxwk\\hf_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = r\"C:\\Users\\venka_5gwzxwk\\hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = r\"C:\\Users\\venka_5gwzxwk\\hf_cache\"\n",
    "\n",
    "os.makedirs(r\"C:\\Users\\venka_5gwzxwk\\hf_cache\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4b8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added: c:\\Users\\venka_5gwzxwk\\OneDrive\\Desktop\\100x\\notebook-lm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Project root added:\", project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474db75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f0f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created: 21\n"
     ]
    }
   ],
   "source": [
    "from ingestion.loader import load_pdf\n",
    "from ingestion.chunker import chunk_text\n",
    "\n",
    "documents = load_pdf(\"../sample-1.pdf\")\n",
    "chunks = chunk_text(documents)\n",
    "\n",
    "print(\"Chunks created:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fc3178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deb09614e6c43909f1ed3ea5d96c444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 21\n",
      "Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from embeddings.embedder import Embedder\n",
    "\n",
    "embedder = Embedder()\n",
    "\n",
    "texts = [chunk[\"text\"] for chunk in chunks]\n",
    "vectors = embedder.embed_texts(texts)\n",
    "\n",
    "print(\"Number of vectors:\", len(vectors))\n",
    "print(\"Vector dimension:\", len(vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abb8001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\venka_5gwzxwk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\venka_5gwzxwk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.13.2-cp310-cp310-win_amd64.whl (18.9 MB)\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.9 MB 1.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.5/18.9 MB 1.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.8/18.9 MB 817.9 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.2 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 2.1/18.9 MB 1.6 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 2.6/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 3.1/18.9 MB 1.9 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.7/18.9 MB 1.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 4.2/18.9 MB 2.0 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.7/18.9 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.2/18.9 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.8/18.9 MB 2.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 6.0/18.9 MB 2.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.6/18.9 MB 2.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 7.1/18.9 MB 2.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 7.6/18.9 MB 2.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 2.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.7/18.9 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 8.9/18.9 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 9.2/18.9 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 9.4/18.9 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 9.7/18.9 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 10.7/18.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 11.5/18.9 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 11.8/18.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.3/18.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 12.8/18.9 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 13.4/18.9 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 14.2/18.9 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.7/18.9 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 15.5/18.9 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.3/18.9 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.8/18.9 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.3/18.9 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.1/18.9 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.9 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.9/18.9 MB 2.3 MB/s  0:00:08\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab3b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore/store.py\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.text_chunks = []\n",
    "\n",
    "    def add(self, vectors: List[List[float]], chunks: List[dict]):\n",
    "        vectors_np = np.array(vectors).astype(\"float32\")\n",
    "        self.index.add(vectors_np)\n",
    "        self.text_chunks.extend(chunks)\n",
    "\n",
    "    def search(self, query_vector: List[float], top_k: int = 3):\n",
    "        query_np = np.array([query_vector]).astype(\"float32\")\n",
    "        distances, indices = self.index.search(query_np, top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            results.append(self.text_chunks[idx])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436435ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors stored: 21\n"
     ]
    }
   ],
   "source": [
    "from vectorstore.store import VectorStore\n",
    "\n",
    "dimension = len(vectors[0])\n",
    "store = VectorStore(dimension)\n",
    "\n",
    "store.add(vectors, chunks)\n",
    "\n",
    "print(\"Vectors stored:\", store.index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17468228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm/generator.py\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class LLMGenerator:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.model_name = model_name\n",
    "        self.url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = requests.post(\n",
    "            self.url,\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return response.json()[\"response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e20f0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, retrieved_chunks):\n",
    "    context_blocks = []\n",
    "\n",
    "    for chunk in retrieved_chunks:\n",
    "        page = chunk[\"metadata\"].get(\"page\", \"Unknown\")\n",
    "        source = chunk[\"metadata\"].get(\"source\", \"Unknown\")\n",
    "        chunk_id = chunk.get(\"chunk_id\", \"Unknown\")\n",
    "\n",
    "        block = f\"\"\"\n",
    "Source: {source}\n",
    "Page: {page}\n",
    "Chunk ID: {chunk_id}\n",
    "\n",
    "{chunk['text']}\n",
    "\"\"\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a document assistant.\n",
    "\n",
    "Answer using ONLY the context below.\n",
    "Cite the page number like (Page X).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d46bdd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d368fdfb594b400d9f453e839ef58065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to (Page 8), \"The trademarks, logos and service marks displayed on the Groww UPI App (“Marks”) are the property of Groww or other respective third parties, as the case may be.\"\n",
      "\n",
      "So, the trademarks displayed on the Groww UPI App will become property of either Groww or other respective third parties.\n"
     ]
    }
   ],
   "source": [
    "from llm.generator import LLMGenerator\n",
    "\n",
    "llm = LLMGenerator()\n",
    "\n",
    "question = \"The trademarks displayed on the Groww UPI App will become property of whoms?\"\n",
    "\n",
    "query_vector = embedder.embed_texts([question])[0]\n",
    "retrieved = store.search(query_vector, top_k=3)\n",
    "\n",
    "prompt = build_prompt(question, retrieved)\n",
    "\n",
    "answer = llm.generate(prompt)\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0acb34dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to (Page 8), \"The trademarks, logos and service marks displayed on the Groww UPI App (“Marks”) are the property of Groww or other respective third parties, as the case may be.\"\n",
      "\n",
      "So, the trademarks displayed on the Groww UPI App will become property of either Groww or other respective third parties.\n",
      "\n",
      "Sources:\n",
      "[Source 1] Page 8 (../sample-1.pdf)\n",
      "[Source 2] Page 10 (../sample-1.pdf)\n",
      "[Source 3] Page 2 (../sample-1.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(answer)\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for i, chunk in enumerate(retrieved):\n",
    "    print(f\"[Source {i+1}] Page {chunk['metadata']['page']} ({chunk['metadata']['source']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daf17e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    query_vector = embedder.embed_texts([question])[0]\n",
    "    retrieved = store.search(query_vector, top_k=3)\n",
    "\n",
    "    prompt = build_prompt(question, retrieved)\n",
    "    answer = llm.generate(prompt)\n",
    "\n",
    "    citations = []\n",
    "\n",
    "    for chunk in retrieved:\n",
    "        citations.append({\n",
    "            \"source\": chunk[\"metadata\"][\"source\"],\n",
    "            \"page\": chunk[\"metadata\"][\"page\"],\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"highlight_text\": chunk[\"text\"][:300]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"citations\": citations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b0f8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.mount(\"/files\", StaticFiles(directory=\"uploads\"), name=\"files\")\n",
    "\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "@app.post(\"/query\")\n",
    "def query_doc(request: QueryRequest):\n",
    "    result = answer_question(request.question)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e21f38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODES = {\n",
    "    \"qa\": \"\"\"\n",
    "You are a document assistant.\n",
    "Answer the question using ONLY the provided context.\n",
    "Cite sources.\n",
    "\"\"\",\n",
    "\n",
    "    \"summary\": \"\"\"\n",
    "You are a professional summarizer.\n",
    "Generate a concise but comprehensive summary of the document using the context.\n",
    "Structure it with headings and bullet points.\n",
    "\"\"\",\n",
    "\n",
    "    \"study_guide\": \"\"\"\n",
    "Create a structured study guide from the document.\n",
    "Include:\n",
    "- Key Concepts\n",
    "- Important Definitions\n",
    "- Important Rules\n",
    "- Practical Implications\n",
    "\"\"\",\n",
    "\n",
    "    \"faq\": \"\"\"\n",
    "Generate 5-10 frequently asked questions and answers\n",
    "based strictly on the document context.\n",
    "\"\"\",\n",
    "\n",
    "    \"timeline\": \"\"\"\n",
    "Extract chronological events from the document\n",
    "and present them in a timeline format.\n",
    "\"\"\",\n",
    "\n",
    "    \"flashcards\": \"\"\"\n",
    "Create 10 flashcards in Q&A format\n",
    "based strictly on the document.\n",
    "Keep them concise and exam-focused.\n",
    "\"\"\",\n",
    "\n",
    "    \"key_arguments\": \"\"\"\n",
    "Extract the main arguments or core themes\n",
    "from the document and explain each briefly.\n",
    "\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bda6606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_mode(mode: str, question: str = None):\n",
    "    if mode not in MODES:\n",
    "        raise ValueError(\"Invalid mode\")\n",
    "\n",
    "    # If question mode, use it\n",
    "    query = question if question else \"Summarize the entire document\"\n",
    "\n",
    "    query_vector = embedder.embed_texts([query])[0]\n",
    "    retrieved = store.search(query_vector, top_k=5)\n",
    "\n",
    "    context_blocks = []\n",
    "    for chunk in retrieved:\n",
    "        context_blocks.append(chunk[\"text\"])\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{MODES[mode]}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if question:\n",
    "        prompt += f\"\\nQuestion:\\n{question}\\n\"\n",
    "\n",
    "    response = llm.generate(prompt)\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"output\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0402531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7661d73b471f4667bc5a233d1051c124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**TERMS AND CONDITIONS**\n",
      "\n",
      "### 1. GENERAL TERMS\n",
      "\n",
      "* GROWW PAY SERVICES PRIVATE LIMITED (Groww) endeavors to execute and process transactions as per the defined process, but shall not be held responsible for non-responsiveness, delay, or failure of systems due to circumstances beyond its control.\n",
      "* The Terms and Conditions may change at any time without notice. You are obligated to exercise independent diligence before arriving at any decision.\n",
      "\n",
      "### 2. USE OF GROWW UPI APP\n",
      "\n",
      "* Groww grants a non-exclusive, limited privilege to access and use the Groww UPI App subject to compliance with these Terms.\n",
      "* You agree not to engage in activities that may adversely affect the use of the Groww UPI App or its systems.\n",
      "\n",
      "### 3. DISCLAIMERS\n",
      "\n",
      "* The Services, including all content, software, functions, materials, and information, are provided \"as is\" without representation or warranty of any kind.\n",
      "* Groww shall not be responsible for service interruptions or breaches of security associated with the transmission of sensitive information through the Services.\n",
      "\n",
      "### 4. INTELLECTUAL PROPERTY RIGHTS\n",
      "\n",
      "* The Content on the Groww UPI App is owned by Groww and protected by copyright, patent, and trademark laws.\n",
      "* You are not permitted to reproduce or distribute the Marks without prior permission from Groww or other respective third parties.\n",
      "\n",
      "### 5. VIOLATION OF THE TERMS\n",
      "\n",
      "* Any violation of these Terms will constitute an unlawful and unfair business practice, causing irreparable harm to Groww.\n",
      "\n",
      "### 6. INDEMNITY & LIABILITY\n",
      "\n",
      "* You shall indemnify and hold harmless Groww, its owner, licensee (as applicable), and their respective officers, directors, agents, and employees from any claim or demand made by any third party due to or arising out of your breach of these Terms.\n",
      "* Groww shall not be liable for direct or indirect damages liabilities, including statutory, consequential, incidental, special, or punitive damages.\n",
      "\n",
      "### 7. TERMINATION\n",
      "\n",
      "* Groww may terminate your arrangement without prior notice if it determines that you have violated the Terms.\n",
      "* You consent to Groww taking injunctive relief as deemed necessary within the said circumstances.\n",
      "\n",
      "### 8. GOVERNING LAW\n",
      "\n",
      "* These Terms and any matters arising under or in connection with these Terms shall be governed by and construed in accordance with the laws of the Republic of India.\n",
      "* The courts in Bengaluru, Karnataka shall have exclusive jurisdiction over all matters connected with the Services.\n",
      "\n",
      "**ACKNOWLEDGMENT**\n",
      "\n",
      "By using the Groww UPI App, you acknowledge that:\n",
      "\n",
      "1. Payment transactions or any communication/offers carried out through Groww UPI App are solely between users.\n",
      "2. Groww is only a facilitator of payment transactions and not a party to these transactions.\n",
      "3. Groww shall not be responsible for products or services purchased or communications made by users.\n"
     ]
    }
   ],
   "source": [
    "result = generate_from_mode(\"summary\")\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2ec19c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c889e8f65eec400f91ee916a98170fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a structured study guide based on the document:\n",
      "\n",
      "**Key Concepts**\n",
      "\n",
      "1. Terms of Service: The terms and conditions under which Groww Pay Services Private Limited (Groww) provides its services.\n",
      "2. Payment Transactions: The transactions carried out through Groww's UPI App, including fund transfer and merchant transactions.\n",
      "3. User Conduct: The behavior expected from users of the Groww UPI App, including compliance with applicable laws and regulations.\n",
      "4. Intellectual Property Rights: The rights owned by Groww to its content, software, functions, materials, and information made available on or accessible through the Groww UPI App.\n",
      "\n",
      "**Important Definitions**\n",
      "\n",
      "1. \"As Is\": A disclaimer that Groww makes no representation or warranty for the services provided.\n",
      "2. \"Non-Exclusive, Limited Privilege\": The right granted to users to access and use the Groww UPI App subject to compliance with the Terms.\n",
      "3. \"PSP\" (Payment Service Provider): A third-party payment service provider that facilitates payment transactions through its platform.\n",
      "\n",
      "**Important Rules**\n",
      "\n",
      "1. User Conduct: Users must comply with applicable laws, regulations, and generally accepted practices or guidelines when using the Groww UPI App.\n",
      "2. Payment Transactions: Groww is only a facilitator of payment transactions and is not a party to these transactions.\n",
      "3. Intellectual Property Rights: Users are not permitted to reproduce or distribute or otherwise use the content, software, functions, materials, and information made available on or accessible through the Groww UPI App without prior permission from Groww.\n",
      "\n",
      "**Practical Implications**\n",
      "\n",
      "1. Compliance with Terms: Users must comply with the Terms of Service when using the Groww UPI App.\n",
      "2. Payment Transaction Integrity: Groww is not responsible for any issues related to payment transactions, and users are solely responsible for their actions.\n",
      "3. Intellectual Property Protection: Groww's intellectual property rights must be respected, and users are not permitted to reproduce or distribute or otherwise use the content, software, functions, materials, and information made available on or accessible through the Groww UPI App without prior permission from Groww.\n",
      "\n",
      "**Additional Information**\n",
      "\n",
      "1. Governing Law: The Terms of Service shall be governed by and construed in accordance with the laws of the Republic of India.\n",
      "2. Dispute Resolution: Any disputes arising under or in connection with this Terms, including the construction, validity, performance or termination thereunder, shall be resolved through arbitration.\n",
      "\n",
      "**Study Questions**\n",
      "\n",
      "1. What are the key concepts and important definitions outlined in the document?\n",
      "2. What are the rules governing user conduct on the Groww UPI App?\n",
      "3. How does the document define \"As Is\" and what implications does this have for users?\n",
      "4. What are the practical implications of complying with the Terms of Service?\n",
      "5. What are the legal frameworks that govern the document?\n"
     ]
    }
   ],
   "source": [
    "result = generate_from_mode(\"study_guide\")\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821330e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f21f32050d40c19003987397466256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = generate_from_mode(\"faq\")\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37fb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
